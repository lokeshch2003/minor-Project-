{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the ranking tasks\n",
    "\n",
    "- randomly select 20 candidate jobs and rank them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(y_true, y_prob):\n",
    "    y_prediction = [0 if i<=0.5 else 1 for i in y_prob]\n",
    "    report = classification_report(y_true,y_prediction,digits=4)\n",
    "    report = report.splitlines()\n",
    "    columns = ['class'] + report[0].split()\n",
    "    col_1, col_2, col_3, col_4, col_5 = [], [], [], [], []\n",
    "    for row in report[1:]:\n",
    "        if len(row.split()) != 0:\n",
    "            row = row.split()\n",
    "            if len(row) < 5:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append('')\n",
    "                col_3.append('')\n",
    "                col_4.append(row[1])\n",
    "                col_5.append(row[2])\n",
    "            elif len(row) > 5:\n",
    "                col_1.append(row[0] + ' ' + row[1])\n",
    "                col_2.append(row[2])\n",
    "                col_3.append(row[3])\n",
    "                col_4.append(row[4])\n",
    "                col_5.append(row[5])\n",
    "            else:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append(row[1])\n",
    "                col_3.append(row[2])\n",
    "                col_4.append(row[3])\n",
    "                col_5.append(row[4])\n",
    "    col_1.append(\"overall\")\n",
    "    col_2.append(precision_score(y_true, y_prediction))\n",
    "    col_3.append(recall_score(y_true, y_prediction))\n",
    "    col_4.append(f1_score(y_true, y_prediction))\n",
    "    col_5.append(roc_auc_score(y_true, y_prob))\n",
    "    result = pd.DataFrame()\n",
    "    result[columns[0]] = col_1\n",
    "    result[columns[1]] = col_2\n",
    "    result[columns[2]] = col_3\n",
    "    result[columns[3]] = col_4\n",
    "    result[columns[4]] = col_5\n",
    "    print(\"——————Test——————\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = pd.read_csv(\"user_set_cleaned.csv\")\n",
    "job_set = pd.read_csv(\"job_set_cleaned.csv\")\n",
    "work_history = pd.read_csv(\"work_history_cleaned.csv\")\n",
    "dataset = pd.read_csv(\"dataset_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "Y_train = np.load(\"Y_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "Y_test = np.load(\"Y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 1 min\n",
    "job_set = job_set.fillna(\" \")\n",
    "job_set[\"word\"] = job_set.Title + job_set.Description + job_set.Requirements\n",
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=5, max_features=100, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(job_set['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_history_tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=1, max_features=50, stop_words='english')\n",
    "word_history_tf_matrix = word_history_tf.fit_transform(work_history.groupby(\"UserID\").JobTitle.sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = user_set[user_set.Split==\"Test\"].UserID.values\n",
    "test_data = dataset[dataset.UserID.isin(test_user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:05<00:00, 51.32it/s]\n"
     ]
    }
   ],
   "source": [
    "ranking_data = pd.DataFrame(columns = [\"UserID\",\"JobID\",\"label\", \"City\", \"State\"])\n",
    "job_id = job_set.JobID.unique().tolist()\n",
    "groups = test_data.groupby(\"UserID\")\n",
    "user_ids = []\n",
    "job_ids = []\n",
    "labels = []\n",
    "City = []\n",
    "State = []\n",
    "for idx, group in tqdm(groups):\n",
    "    size = 99\n",
    "    exist_job = group.JobID.unique().tolist()\n",
    "    candidate_job = [i for i in job_id if i not in exist_job ]\n",
    "    sample_job = random.sample(range(0,len(candidate_job)),size)\n",
    "    user_ids.extend([idx] * (size+1))\n",
    "    job_ids.append(exist_job[0])\n",
    "    job_ids.extend([candidate_job[i] for i in sample_job])\n",
    "    labels.append(1)\n",
    "    labels.extend([0] * (size))\n",
    "    City.append(group.City.values[0])\n",
    "    State.append(group.State.values[0])\n",
    "    jobs = job_set[job_set.JobID.isin([candidate_job[i] for i in sample_job])]\n",
    "    \n",
    "    City.extend([0 if i!=group.City.values[0] else a for i in jobs.City.values.tolist()])\n",
    "    State.extend([0 if i!=group.State.values[0] else a for i in jobs.State.values.tolist()])\n",
    "    \n",
    "ranking_data.UserID = user_ids\n",
    "ranking_data.JobID = job_ids\n",
    "ranking_data.label = labels\n",
    "ranking_data.City = City\n",
    "ranking_data.State = State\n",
    "# ranking_data.to_csv(\"ranking_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hit_rate(model, N):\n",
    "    hit = 0\n",
    "    groups = ranking_data.groupby(\"UserID\")\n",
    "    for u_id, group in tqdm(groups):\n",
    "        X = np.zeros((1,158))\n",
    "        user = user_set[user_set.UserID==u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                                \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "        u_idx = user.index.values[0]\n",
    "        user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx,:].toarray()),axis=1)\n",
    "        job_id_list = group.JobID.values\n",
    "        jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "        j_idx = jobs.index.values\n",
    "        f = []\n",
    "        for i in j_idx:\n",
    "            feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "            f.append(feature)\n",
    "        feature = np.concatenate((group[[\"City\",\"State\"]].values, np.array(f)),axis=1)\n",
    "        X = np.concatenate((X, feature), axis=0)\n",
    "        result = model.predict_proba(X[1:])\n",
    "#         result = model.predict(X[1:])\n",
    "        a = -np.sort(-result[:,1])\n",
    "        idx = np.argwhere(a==result[0,1])[0][0]\n",
    "        if idx <= N-1:\n",
    "            hit += 1\n",
    "    return hit/len(test_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test models\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall f1-score   support\n",
      "0             0    0.6353    0.6414   0.6383       527\n",
      "1             1    0.6379    0.6319   0.6349       527\n",
      "2      accuracy                       0.6366      1054\n",
      "3     macro avg    0.6366    0.6366   0.6366      1054\n",
      "4  weighted avg    0.6366    0.6366   0.6366      1054\n",
      "5       overall  0.637931  0.631879  0.63489  0.712698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create the imputer with a chosen strategy (mean, median, most_frequent, or constant)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute the missing values in the training and test sets\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Create and fit the RandomForestClassifier model\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train_imputed, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict_proba(X_test_imputed)[:, 1]\n",
    "\n",
    "# Function to show results (assuming show_result is defined)\n",
    "show_result(Y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:09<00:00, 27.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:09<00:00, 27.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:09<00:00, 27.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:09<00:00, 27.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03461538461538462,\n",
       " 0.1346153846153846,\n",
       " 0.2230769230769231,\n",
       " 0.38076923076923075)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the pipeline with imputer and RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Test hit rate using the pipeline\n",
    "test_hit_rate(pipeline, 1), test_hit_rate(pipeline, 5), test_hit_rate(pipeline, 10), test_hit_rate(pipeline, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score  support\n",
      "0             0    0.5251    0.5351    0.5301      527\n",
      "1             1    0.5261    0.5161    0.5211      527\n",
      "2      accuracy                        0.5256     1054\n",
      "3     macro avg    0.5256    0.5256    0.5256     1054\n",
      "4  weighted avg    0.5256    0.5256    0.5256     1054\n",
      "5       overall  0.526112  0.516129  0.521073  0.52463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 60.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 59.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 61.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 63.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate for top 1: 0.015384615384615385\n",
      "Hit rate for top 5: 0.10384615384615385\n",
      "Hit rate for top 10: 0.15384615384615385\n",
      "Hit rate for top 20: 0.28076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_hit_rate_linearRegr(model, N):\n",
    "    hit = 0\n",
    "    groups = ranking_data.groupby(\"UserID\")\n",
    "    for u_id, group in tqdm(groups):\n",
    "        X = np.zeros((1,158))\n",
    "        user = user_set[user_set.UserID==u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                                \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "        u_idx = user.index.values[0]\n",
    "        user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx,:].toarray()), axis=1)\n",
    "        job_id_list = group.JobID.values\n",
    "        jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "        j_idx = jobs.index\n",
    "        f = []\n",
    "        for i in j_idx:\n",
    "            feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "            f.append(feature)\n",
    "        feature = np.concatenate((group[[\"City\",\"State\"]].values, np.array(f)), axis=1)\n",
    "        X = np.concatenate((X, feature), axis=0)\n",
    "        result = model.predict(X[1:])\n",
    "        a = -np.sort(-result)\n",
    "        idx = np.argwhere(a == result[0])[0][0]\n",
    "        if idx <= N-1:\n",
    "            hit += 1\n",
    "    return hit / len(groups)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, Y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "show_result(Y_test, y_pred)\n",
    "\n",
    "# Test hit rates\n",
    "hit_rate_1 = test_hit_rate_linearRegr(pipeline, 1)\n",
    "hit_rate_5 = test_hit_rate_linearRegr(pipeline, 5)\n",
    "hit_rate_10 = test_hit_rate_linearRegr(pipeline, 10)\n",
    "hit_rate_20 = test_hit_rate_linearRegr(pipeline, 20)\n",
    "\n",
    "print(f\"Hit rate for top 1: {hit_rate_1}\")\n",
    "print(f\"Hit rate for top 5: {hit_rate_5}\")\n",
    "print(f\"Hit rate for top 10: {hit_rate_10}\")\n",
    "print(f\"Hit rate for top 20: {hit_rate_20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/260 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_hit_rate_linearRegr(linear_r,\u001b[38;5;241m1\u001b[39m), test_hit_rate_linearRegr(linear_r,\u001b[38;5;241m5\u001b[39m), test_hit_rate_linearRegr(linear_r,\u001b[38;5;241m10\u001b[39m), test_hit_rate_linearRegr(linear_r, \u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m, in \u001b[0;36mtest_hit_rate_linearRegr\u001b[1;34m(model, N)\u001b[0m\n\u001b[0;32m     23\u001b[0m feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((group[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues, np\u001b[38;5;241m.\u001b[39marray(f)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X, feature), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m     26\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;241m-\u001b[39mresult)\n\u001b[0;32m     27\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(a \u001b[38;5;241m==\u001b[39m result[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:367\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision   recall f1-score   support\n",
      "0             0    0.5333   0.5009   0.5166       527\n",
      "1             1    0.5295   0.5617   0.5451       527\n",
      "2      accuracy                      0.5313      1054\n",
      "3     macro avg    0.5314   0.5313   0.5309      1054\n",
      "4  weighted avg    0.5314   0.5313   0.5309      1054\n",
      "5       overall  0.529517  0.56167  0.54512  0.527964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 60.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 62.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 63.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 63.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate for top 1: 0.6461538461538462\n",
      "Hit rate for top 5: 0.6461538461538462\n",
      "Hit rate for top 10: 0.6461538461538462\n",
      "Hit rate for top 20: 0.6461538461538462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # You can change 'mean' to 'median' or another strategy if needed\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "pipeline.fit(X_train, Y_train)\n",
    "y_pred = pipeline.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])\n",
    "def test_hit_rate_linearRegr(model, N):\n",
    "    hit = 0\n",
    "    groups = ranking_data.groupby(\"UserID\")\n",
    "    for u_id, group in tqdm(groups):\n",
    "        X = np.zeros((1, 158))\n",
    "        user = user_set[user_set.UserID == u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                                  \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "        u_idx = user.index.values[0]\n",
    "        user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx,:].toarray()), axis=1)\n",
    "        job_id_list = group.JobID.values\n",
    "        jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "        j_idx = jobs.index\n",
    "        f = []\n",
    "        for i in j_idx:\n",
    "            feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "            f.append(feature)\n",
    "        feature = np.concatenate((group[[\"City\", \"State\"]].values, np.array(f)), axis=1)\n",
    "        X = np.concatenate((X, feature), axis=0)\n",
    "        result = model.predict(X[1:])\n",
    "        a = -np.sort(-result)\n",
    "        idx = np.argwhere(a == result[0])[0][0]\n",
    "        if idx <= N-1:\n",
    "            hit += 1\n",
    "    return hit / len(groups)\n",
    "\n",
    "# Test hit rates\n",
    "hit_rate_1 = test_hit_rate_linearRegr(pipeline, 1)\n",
    "hit_rate_5 = test_hit_rate_linearRegr(pipeline, 5)\n",
    "hit_rate_10 = test_hit_rate_linearRegr(pipeline, 10)\n",
    "hit_rate_20 = test_hit_rate_linearRegr(pipeline, 20)\n",
    "\n",
    "print(f\"Hit rate for top 1: {hit_rate_1}\")\n",
    "print(f\"Hit rate for top 5: {hit_rate_5}\")\n",
    "print(f\"Hit rate for top 10: {hit_rate_10}\")\n",
    "print(f\"Hit rate for top 20: {hit_rate_20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.5024    0.5882    0.5420       527\n",
      "1             1    0.5034    0.4175    0.4564       527\n",
      "2      accuracy                        0.5028      1054\n",
      "3     macro avg    0.5029    0.5028    0.4992      1054\n",
      "4  weighted avg    0.5029    0.5028    0.4992      1054\n",
      "5       overall  0.503432  0.417457  0.456432  0.525833\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_leaf_nodes=1500,random_state=0)\n",
    "dt.fit(X_train,Y_train)\n",
    "y_pred = dt.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 62.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 62.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 64.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 64.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03076923076923077,\n",
       " 0.07692307692307693,\n",
       " 0.23076923076923078,\n",
       " 0.5423076923076923)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hit_rate(dt,1), test_hit_rate(dt,5), test_hit_rate(dt,10), test_hit_rate(dt,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.5174    0.5655    0.5403       527\n",
      "1             1    0.5209    0.4725    0.4955       527\n",
      "2      accuracy                        0.5190      1054\n",
      "3     macro avg    0.5191    0.5190    0.5179      1054\n",
      "4  weighted avg    0.5191    0.5190    0.5179      1054\n",
      "5       overall  0.520921  0.472486  0.495522  0.525539\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize SimpleImputer with a strategy (e.g., 'mean', 'median', 'most_frequent')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer on the training data and transform both training and test data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Initialize and fit Gaussian Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_imputed, Y_train)\n",
    "\n",
    "# Predict probabilities for test data\n",
    "y_pred = nb.predict_proba(X_test_imputed)\n",
    "\n",
    "# Show results\n",
    "show_result(Y_test, y_pred[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▍                                                                             | 11/260 [00:00<00:04, 58.75it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_hit_rate(nb,\u001b[38;5;241m1\u001b[39m), test_hit_rate(nb,\u001b[38;5;241m5\u001b[39m), test_hit_rate(nb,\u001b[38;5;241m10\u001b[39m), test_hit_rate(nb,\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mtest_hit_rate\u001b[1;34m(model, N)\u001b[0m\n\u001b[0;32m     17\u001b[0m         feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((group[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues, np\u001b[38;5;241m.\u001b[39marray(f)),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m         X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X, feature), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m         result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#         result = model.predict(X[1:])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;241m-\u001b[39mresult[:,\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:144\u001b[0m, in \u001b[0;36m_BaseNB.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Return probability estimates for the test vector X.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m        order, as they appear in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_log_proba(X))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:122\u001b[0m, in \u001b[0;36m_BaseNB.predict_log_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mReturn log-probability estimates for the test vector X.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    order, as they appear in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(X)\n\u001b[0;32m    123\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# normalize by P(x) = P(f_1, ..., f_n)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:269\u001b[0m, in \u001b[0;36mGaussianNB._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nGaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score  support\n",
      "0             0    0.5081    0.5370    0.5221      527\n",
      "1             1    0.5091    0.4801    0.4941      527\n",
      "2      accuracy                        0.5085     1054\n",
      "3     macro avg    0.5086    0.5085    0.5081     1054\n",
      "4  weighted avg    0.5086    0.5085    0.5081     1054\n",
      "5       overall  0.509054  0.480076  0.494141  0.50184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize SimpleImputer with a strategy (e.g., 'mean', 'median', 'most_frequent')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer on the training data and transform both training and test data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Initialize and fit AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "ada.fit(X_train_imputed, Y_train)\n",
    "\n",
    "# Predict probabilities for test data\n",
    "y_pred = ada.predict_proba(X_test_imputed)\n",
    "\n",
    "# Show results\n",
    "show_result(Y_test, y_pred[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▍                                                                             | 11/260 [00:00<00:07, 34.85it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nAdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_hit_rate(ada,\u001b[38;5;241m1\u001b[39m), test_hit_rate(ada,\u001b[38;5;241m5\u001b[39m), test_hit_rate(ada,\u001b[38;5;241m10\u001b[39m), test_hit_rate(ada,\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mtest_hit_rate\u001b[1;34m(model, N)\u001b[0m\n\u001b[0;32m     17\u001b[0m         feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((group[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues, np\u001b[38;5;241m.\u001b[39marray(f)),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m         X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X, feature), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m         result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#         result = model.predict(X[1:])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;241m-\u001b[39mresult[:,\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:889\u001b[0m, in \u001b[0;36mAdaBoostClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones((_num_samples(X), \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 889\u001b[0m decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_proba_from_decision(decision, n_classes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:771\u001b[0m, in \u001b[0;36mAdaBoostClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;124;03m    class in ``classes_``, respectively.\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    770\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 771\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(X)\n\u001b[0;32m    773\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[0;32m    774\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:101\u001b[0m, in \u001b[0;36mBaseWeightBoosting._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# Only called to validate X in non-fit methods, therefore reset=False\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    102\u001b[0m         X,\n\u001b[0;32m    103\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    104\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    105\u001b[0m         allow_nd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    107\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    108\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nAdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbdt = GradientBoostingClassifier(max_depth=10, random_state=0, verbose=1)\n",
    "gbdt.fit(X_train,Y_train)\n",
    "y_pred = gbdt.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hit_rate(gbdt,1),  test_hit_rate(gbdt,5), test_hit_rate(gbdt,10), test_hit_rate(gbdt,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
