{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "- TF-IDF vectors to represent the texts.\n",
    "- Use several machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = pd.read_csv(\"user_set.csv\")\n",
    "job_set = pd.read_csv(\"job_set_cleaned.csv\")\n",
    "work_history = pd.read_csv(\"work_history.csv\")\n",
    "dataset = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TF-IDF vectors for text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 1 min\n",
    "job_set = job_set.fillna(\" \")\n",
    "job_set[\"word\"] = job_set.Title + job_set.Description + job_set.Requirements\n",
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=5, max_features=100, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(job_set['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filter out users with more than 10 applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6765"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sorted(dict(dataset.UserID.value_counts()).items(), key=lambda x: x[1], reverse=True)\n",
    "exclude_user_id = [i[0] for i in temp if i [1]>=10]\n",
    "len(exclude_user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[~dataset.UserID.isin(exclude_user_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select data in ```work_history,user_set```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = dataset.UserID.unique()\n",
    "work_history = work_history[work_history.UserID.isin(user_id)]\n",
    "user_set = user_set[user_set.UserID.isin(user_id)]\n",
    "user_set.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- drop duplicates in ```work_history```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_history = work_history.drop(columns=[\"Sequence\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_history_tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=1, max_features=50, stop_words='english')\n",
    "word_history_tf_matrix = word_history_tf.fit_transform(work_history.groupby(\"UserID\").JobTitle.sum().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deal with the user set and the job set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = user_set.drop(columns=[\"Country\",\"ZipCode\",\"Major\",\"GraduationDate\",\"WindowID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ```user_set``` \n",
    "- label encoding for ```DegreeType```\n",
    "- one-hot encoding for ```State```\n",
    "- binary labels for Currently ```Employed/ManagedOthers```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_set = pd.get_dummies(user_set, columns=[\"State\"])\n",
    "user_set.replace({\"CurrentlyEmployed\":{\"Yes\":1,\"No\":0}}, inplace=True)\n",
    "user_set.replace({\"ManagedOthers\":{\"Yes\":1,\"No\":0}}, inplace=True)\n",
    "user_set.replace({\"DegreeType\":{\"None\":0,\"High School\":1, \"Vocational\":2, \"Associate's\":3, \"Bachelor's\":4, \"Master's\":5, \"PhD\":6}}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Split</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>DegreeType</th>\n",
       "      <th>WorkHistoryCount</th>\n",
       "      <th>TotalYearsExperience</th>\n",
       "      <th>CurrentlyEmployed</th>\n",
       "      <th>ManagedOthers</th>\n",
       "      <th>ManagedHowMany</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>Test</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>Train</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>OH</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>Train</td>\n",
       "      <td>Brick</td>\n",
       "      <td>NJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133</td>\n",
       "      <td>Train</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>DE</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>182</td>\n",
       "      <td>Train</td>\n",
       "      <td>Lenexa</td>\n",
       "      <td>KS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18741</th>\n",
       "      <td>1471625</td>\n",
       "      <td>Train</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>IN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18742</th>\n",
       "      <td>1471661</td>\n",
       "      <td>Train</td>\n",
       "      <td>Shartlesville</td>\n",
       "      <td>PA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18743</th>\n",
       "      <td>1471838</td>\n",
       "      <td>Train</td>\n",
       "      <td>Peoria</td>\n",
       "      <td>AZ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18744</th>\n",
       "      <td>1471948</td>\n",
       "      <td>Train</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18745</th>\n",
       "      <td>1472019</td>\n",
       "      <td>Train</td>\n",
       "      <td>Erlanger</td>\n",
       "      <td>KY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18746 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        UserID  Split           City State  DegreeType  WorkHistoryCount  \\\n",
       "0           13   Test   Philadelphia    PA         4.0                 6   \n",
       "1           64  Train       Columbus    OH         5.0                 3   \n",
       "2          101  Train          Brick    NJ         1.0                 1   \n",
       "3          133  Train     Wilmington    DE         4.0                 6   \n",
       "4          182  Train         Lenexa    KS         1.0                 3   \n",
       "...        ...    ...            ...   ...         ...               ...   \n",
       "18741  1471625  Train   Indianapolis    IN         4.0                 4   \n",
       "18742  1471661  Train  Shartlesville    PA         4.0                 1   \n",
       "18743  1471838  Train         Peoria    AZ         5.0                 3   \n",
       "18744  1471948  Train       Glendale    AZ         1.0                 4   \n",
       "18745  1472019  Train       Erlanger    KY         1.0                 1   \n",
       "\n",
       "       TotalYearsExperience  CurrentlyEmployed  ManagedOthers  ManagedHowMany  \n",
       "0                       5.0                  1              0               0  \n",
       "1                      22.0                  1              0               0  \n",
       "2                       2.0                  0              1               4  \n",
       "3                       9.0                  1              1               6  \n",
       "4                       5.0                  1              1              10  \n",
       "...                     ...                ...            ...             ...  \n",
       "18741                   4.0                  1              1              10  \n",
       "18742                   3.0                  0              0               0  \n",
       "18743                   8.0                  1              0               0  \n",
       "18744                   6.0                  0              0               0  \n",
       "18745                   4.0                  0              0               0  \n",
       "\n",
       "[18746 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add binary labels into the dataset, indicating that whether the user and job are in the same city/state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18746/18746 [01:49<00:00, 171.92it/s]\n"
     ]
    }
   ],
   "source": [
    "city = []\n",
    "state = []\n",
    "groups = dataset.groupby(\"UserID\")\n",
    "for idx, group in tqdm(groups):\n",
    "    user_city = user_set[user_set.UserID==idx][\"City\"].values\n",
    "    user_state = user_set[user_set.UserID==idx][\"State\"].values\n",
    "    job_id_list = group.JobID.values\n",
    "    job_city = job_set[job_set.JobID.isin(job_id_list)][\"City\"].values\n",
    "    job_state = job_set[job_set.JobID.isin(job_id_list)][\"State\"].values\n",
    "    city.extend([0 if i!=user_city else 1 for i in job_city])\n",
    "    state.extend([0 if i!=user_state else 1 for i in job_state])\n",
    "dataset[\"City\"] = city\n",
    "dataset[\"State\"] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set.to_csv(\"user_set_cleaned.csv\", index=False)\n",
    "dataset.to_csv(\"dataset_cleaned.csv\", index=False)\n",
    "work_history.to_csv(\"work_history_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build the training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = user_set[user_set.Split==\"Train\"].UserID.values\n",
    "test_user = user_set[user_set.Split==\"Test\"].UserID.values\n",
    "train_data = dataset[dataset.UserID.isin(train_user)]\n",
    "test_data = dataset[dataset.UserID.isin(test_user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18486/18486 [11:39<00:00, 26.44it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = train_data.groupby(\"UserID\")\n",
    "X_train = np.zeros((1,158))\n",
    "Y_train = []\n",
    "for u_id, group in tqdm(groups):\n",
    "    user = user_set[user_set.UserID==u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                            \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "    u_idx = user.index.values[0]\n",
    "    user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx,:].toarray()),axis=1)\n",
    "    job_id_list = group.JobID.values\n",
    "    jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "    j_idx = jobs.index.values\n",
    "    f = []\n",
    "    for i in j_idx:\n",
    "        feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "        f.append(feature)\n",
    "    feature = np.concatenate((group[[\"City\",\"State\"]].values, np.array(f)),axis=1)\n",
    "    X_train = np.concatenate((X_train, feature), axis=0)\n",
    "    Y_train.extend(group.label.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70669, 158), 70668)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 260/260 [00:02<00:00, 87.06it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = test_data.groupby(\"UserID\")\n",
    "X_test = np.zeros((1,158))\n",
    "Y_test = []\n",
    "for u_id, group in tqdm(groups):\n",
    "    user = user_set[user_set.UserID==u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                            \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "    u_idx = user.index.values[0]\n",
    "    user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx,:].toarray()),axis=1)\n",
    "    job_id_list = group.JobID.values\n",
    "    jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "    j_idx = jobs.index.values\n",
    "    f = []\n",
    "    for i in j_idx:\n",
    "        feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "        f.append(feature)\n",
    "    feature = np.concatenate((group[[\"City\",\"State\"]].values, np.array(f)),axis=1)\n",
    "    X_test = np.concatenate((X_test, feature), axis=0)\n",
    "    Y_test.extend(group.label.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1055, 158), 1054)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\",X_train[1:,])\n",
    "np.save(\"Y_train.npy\",np.array(Y_train))\n",
    "np.save(\"X_test.npy\",X_test[1:,])\n",
    "np.save(\"Y_test.npy\",np.array(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Construct models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(y_true, y_prediction):\n",
    "    report = classification_report(y_true,y_prediction,digits=4)\n",
    "    report = report.splitlines()\n",
    "    columns = ['class'] + report[0].split()\n",
    "    col_1, col_2, col_3, col_4, col_5 = [], [], [], [], []\n",
    "    for row in report[1:]:\n",
    "        if len(row.split()) != 0:\n",
    "            row = row.split()\n",
    "            if len(row) < 5:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append('')\n",
    "                col_3.append('')\n",
    "                col_4.append(row[1])\n",
    "                col_5.append(row[2])\n",
    "            elif len(row) > 5:\n",
    "                col_1.append(row[0] + ' ' + row[1])\n",
    "                col_2.append(row[2])\n",
    "                col_3.append(row[3])\n",
    "                col_4.append(row[4])\n",
    "                col_5.append(row[5])\n",
    "            else:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append(row[1])\n",
    "                col_3.append(row[2])\n",
    "                col_4.append(row[3])\n",
    "                col_5.append(row[4])\n",
    "    col_1.append(\"overall\")\n",
    "    col_2.append(precision_score(y_true, y_prediction))\n",
    "    col_3.append(recall_score(y_true, y_prediction))\n",
    "    col_4.append(f1_score(y_true, y_prediction))\n",
    "    col_5.append(roc_auc_score(y_true, y_prediction))\n",
    "    result = pd.DataFrame()\n",
    "    result[columns[0]] = col_1\n",
    "    result[columns[1]] = col_2\n",
    "    result[columns[2]] = col_3\n",
    "    result[columns[3]] = col_4\n",
    "    result[columns[4]] = col_5\n",
    "    print(\"â€”â€”â€”â€”â€”â€”Testâ€”â€”â€”â€”â€”â€”\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "Y_train = np.load(\"Y_train.npy\")\n",
    "Y_texs = np.load(\"Y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”Testâ€”â€”â€”â€”â€”â€”\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.5251    0.5351    0.5301       527\n",
      "1             1    0.5261    0.5161    0.5211       527\n",
      "2      accuracy                        0.5256      1054\n",
      "3     macro avg    0.5256    0.5256    0.5256      1054\n",
      "4  weighted avg    0.5256    0.5256    0.5256      1054\n",
      "5       overall  0.526112  0.516129  0.521073  0.525617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create the imputer and fill missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Fit the model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_imputed, Y_train)\n",
    "\n",
    "# Predict and transform the predictions\n",
    "y_pred = lr.predict(X_test_imputed)\n",
    "y_pred = [0 if i < 0.5 else 1 for i in y_pred]\n",
    "show_result(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”Testâ€”â€”â€”â€”â€”â€”\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.5261    0.5351    0.5306       527\n",
      "1             1    0.5270    0.5180    0.5225       527\n",
      "2      accuracy                        0.5266      1054\n",
      "3     macro avg    0.5266    0.5266    0.5265      1054\n",
      "4  weighted avg    0.5266    0.5266    0.5265      1054\n",
      "5       overall  0.527027  0.518027  0.522488  0.526565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Create the imputer with a chosen strategy (mean, median, most_frequent, or constant)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute the missing values in the training and test sets\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Create and fit the Logistic Regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_imputed, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr.predict(X_test_imputed)\n",
    "\n",
    "# Function to show results (assuming show_result is defined)\n",
    "show_result(Y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”Testâ€”â€”â€”â€”â€”â€”\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.5174    0.5655    0.5403       527\n",
      "1             1    0.5209    0.4725    0.4955       527\n",
      "2      accuracy                        0.5190      1054\n",
      "3     macro avg    0.5191    0.5190    0.5179      1054\n",
      "4  weighted avg    0.5191    0.5190    0.5179      1054\n",
      "5       overall  0.520921  0.472486  0.495522  0.518975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create the imputer with a chosen strategy (mean, median, most_frequent, or constant)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute the missing values in the training and test sets\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Create and fit the Gaussian Naive Bayes model\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_imputed, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb.predict(X_test_imputed)\n",
    "\n",
    "# Function to show results (assuming show_result is defined)\n",
    "show_result(Y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”Testâ€”â€”â€”â€”â€”â€”\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.5361    0.7476    0.6244       527\n",
      "1             1    0.5831    0.3529    0.4397       527\n",
      "2      accuracy                        0.5503      1054\n",
      "3     macro avg    0.5596    0.5503    0.5321      1054\n",
      "4  weighted avg    0.5596    0.5503    0.5321      1054\n",
      "5       overall  0.583072  0.352941  0.439716  0.550285\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, Y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "show_result(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”Testâ€”â€”â€”â€”â€”â€”\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.6241    0.6395    0.6317       527\n",
      "1             1    0.6304    0.6148    0.6225       527\n",
      "2      accuracy                        0.6271      1054\n",
      "3     macro avg    0.6272    0.6271    0.6271      1054\n",
      "4  weighted avg    0.6272    0.6271    0.6271      1054\n",
      "5       overall   0.63035  0.614801  0.622478  0.627135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the imputer with a chosen strategy (mean, median, most_frequent, or constant)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute the missing values in the training and test sets\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Create and fit the RandomForestClassifier model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_imputed, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test_imputed)\n",
    "\n",
    "# Function to show results (assuming show_result is defined)\n",
    "show_result(Y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
